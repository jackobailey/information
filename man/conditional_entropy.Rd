% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/information_theory.R
\name{conditional_entropy}
\alias{conditional_entropy}
\title{Compute Conditional Entropy}
\usage{
conditional_entropy(p = NULL, margin = 1, base = 2)
}
\arguments{
\item{p}{A matrix of probabilities.}

\item{margin}{Which variable to condition on? Either 1 (rows) or 2 (columns). Defaults to 1 (rows)}

\item{base}{Which log base to use? Three are most typical. Base 2 for "bits", base \emph{e} for "nats", and base 10 for "hartleys". Defaults to base 2 (bits).}
}
\value{
\code{conditional_entropy()} takes a matrix of probabilities as its input. It then returns the matrix's entropy as its output.
}
\description{
Conditional entropy quantifies the uncertainty remaining in one variable when another variable is known. It measures the average amount of information required to describe the first variable given the value of the second. Lower conditional entropy suggests a more predictable relationship between the variables, whereas higher conditional entropy indicates greater uncertainty.

\code{conditional_entropy()} provides a simple way to compute conditional entropy in \code{R}.
}
\references{
Shannon, C. E. (1948). A mathematical theory of communication, \emph{The Bell System Technical Journal}. 27(3), pp. 379â€“423.
}
