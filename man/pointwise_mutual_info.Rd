% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/information_theory.R
\name{pointwise_mutual_info}
\alias{pointwise_mutual_info}
\title{Compute Pointwise Mutual Information}
\usage{
pointwise_mutual_info(p = NULL, base = 2)
}
\arguments{
\item{p}{A matrix of probabilities.}

\item{base}{Which log base to use? Three are most typical. Base 2 for "bits", base \emph{e} for "nats", and base 10 for "hartleys". Defaults to base 2 (bits).}
}
\value{
\code{pointwise_mutual_info()} takes a matrix of probabilities as its input. It then returns each element's pointwise mutual information as its output.
}
\description{
Pointwise mutual information measures how much information the outcomes of a set of variables' share. Unlike many other information theoretic quantities, it can be either positive or negative. A positive value indicates events that tend to occur together. A negative value suggests events that are likely independent. The expected value of the pointwise mutual information of a set of variables equals its mutual information.

\code{pointwise_mutual_info} provides a simple way to compute pointwise mutual information in \code{R}.
}
\references{
Shannon, C. E. (1948). A mathematical theory of communication, \emph{The Bell System Technical Journal}. 27(3), pp. 379â€“423.
}
