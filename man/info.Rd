% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/information_theory.R
\name{info}
\alias{info}
\title{Compute Information Content}
\usage{
info(p = NULL, base = 2)
}
\arguments{
\item{p}{A numeric vector or matrix of probabilities.mc}

\item{base}{Which log base to use? Three are most typical. Base 2 for "bits", base \emph{e} for "nats", and base 10 for "hartleys". Defaults to base 2 (bits).}
}
\value{
\code{info()} takes a vector or matrix of probabilities as its input. It then returns the information content of each element as its output.
}
\description{
Information content measures our surprise in some outcome. Uncommon outcomes have a higher information content. Common outcomes, instead, have a lower information content. This is because uncommon outcomes do more to reduce our uncertainty than do common ones.

\code{info()} provides a simple way to compute information content in \code{R}.
}
\references{
Shannon, C. E. (1948). A mathematical theory of communication, \emph{The Bell System Technical Journal}. 27(3), pp. 379â€“423.
}
