% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/information_theory.R
\name{mutual_info}
\alias{mutual_info}
\title{Compute Mutual Information}
\usage{
mutual_info(p = NULL, base = 2)
}
\arguments{
\item{p}{A matrix of probabilities.}

\item{base}{Which log base to use? Three are most typical. Base 2 for "bits", base \emph{e} for "nats", and base 10 for "hartleys". Defaults to base 2 (bits).}
}
\value{
\code{mutual_info()} takes a matrix of probabilities as its input. It then returns the matrix's mutual information as its output.
}
\description{
Mutual information measures the amount of information shared between two variables. It quantifies how much knowing the value of one variable reduces the uncertainty about the other. When the variables are independent, the mutual information is zero, indicating no shared information. Conversely, a higher mutual information value indicates a stronger dependency between the variables.

\code{mutual_info()} provides a simple way to compute mutual information in \code{R}.
}
\references{
Shannon, C. E. (1948). A mathematical theory of communication, \emph{The Bell System Technical Journal}. 27(3), pp. 379â€“423.
}
