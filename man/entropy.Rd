% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/information_theory.R
\name{entropy}
\alias{entropy}
\title{Compute Entropy}
\usage{
entropy(p = NULL, base = 2)
}
\arguments{
\item{p}{A numeric vector of probabilities.}

\item{base}{Which log base to use? Three are most typical. Base 2 for "bits", base \emph{e} for "nats", and base 10 for "hartleys". Defaults to base 2 (bits).}
}
\value{
\code{entropy()} takes a vector of probabilities as its input. It then returns the vector's entropy as its output.
}
\description{
Information entropy quantifies the uncertainty in a given variable. It measures the average amount of information across each of the variable's possible outcomes. For example, a fair coin toss would have an entropy of \eqn{0.5 \times I(0.5) + 0.5 \times I(0.5) = 1}. Systems with higher entropy are more unpredictable. Systems with lower entropy, instead, are less unpredictable. Importantly, a system's entropy also equals the fewest bits needed to compress it without losing information.

\code{entropy()} provides a simple way to compute information entropy in \code{R}.
}
\references{
Shannon, C. E. (1948). A mathematical theory of communication, \emph{The Bell System Technical Journal}. 27(3), pp. 379â€“423.
}
