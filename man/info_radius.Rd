% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/information_theory.R
\name{info_radius}
\alias{info_radius}
\title{Compute Information Radius}
\usage{
info_radius(p = NULL, q = NULL, base = 2)
}
\arguments{
\item{p}{A numeric vector of probabilities.}

\item{q}{A numeric vector of probabilities.}

\item{base}{Which log base to use? Three are most typical. Base 2 for "bits", base \emph{e} for "nats", and base 10 for "hartleys". Defaults to base 2 (bits).}
}
\value{
\code{info_radius()} takes two numeric vectors as its input. It then returns the vectors' relative entropy as its output.
}
\description{
Information radius, also known as Jensen-Shannon divergence, is a symmetrised version of the Kullback-Leibler divergence. It quantifies the similarity between two probability distributions and represents the average "distance" between them. The information radius ranges from 0 to 1, where 0 indicates identical distributions, and 1 suggests completely different distributions.

\code{info_radius()} provides a simple way to compute the Jensen-Shannon divergence in \code{R}.
}
\references{
Shannon, C. E. (1948). A mathematical theory of communication, \emph{The Bell System Technical Journal}. 27(3), pp. 379â€“423.
}
